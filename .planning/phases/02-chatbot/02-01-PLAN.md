---
phase: 02-chatbot
plan: 01
type: execute
wave: 1
depends_on: []
files_modified: []
autonomous: true
requirements: [CHAT-01, CHAT-02, CHAT-03, CHAT-07]

must_haves:
  truths:
    - "User can send natural language messages and receive streaming responses"
    - "System extracts structured requirements from conversation"
    - "Conversation history persists during the session"
  artifacts:
    - path: "aadap/services/chat_service.py"
      provides: "Chat service with LLM integration and requirement extraction"
      exports: ["ChatService"]
    - path: "aadap/api/chat.py"
      provides: "Chat API endpoints with SSE streaming"
      exports: ["router"]
    - path: "aadap/models/chat.py"
      provides: "Chat session and message models"
      contains: "class ChatSession"
  key_links:
    - from: "aadap/api/chat.py"
      to: "aadap/services/chat_service.py"
      via: "dependency injection"
      pattern: "ChatService"
    - from: "aadap/services/chat_service.py"
      to: "Azure OpenAI"
      via: "LLM client"
      pattern: "azure_openai|AsyncOpenAI"
---

<objective>
Create the chat backend service with SSE streaming and requirement extraction.

Purpose: Enable users to have natural language conversations that extract structured task requirements.
Output: Chat API endpoints, service layer, and data models for session management.
</objective>

<execution_context>
@C:/Users/parth/.config/opencode/get-shit-done/workflows/execute-plan.md
@C:/Users/parth/.config/opencode/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-chatbot/02-CONTEXT.md
@.planning/REQUIREMENTS.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create chat data models</name>
  <files>aadap/models/chat.py</files>
  <action>
    Create Pydantic models for chat functionality:
    
    1. **ChatMessage** model:
       - `role: Literal["user", "assistant", "system"]`
       - `content: str`
       - `timestamp: datetime`
       - `extracted_requirements: Optional[ExtractedRequirements]` (for assistant messages with confidence)
    
    2. **ExtractedRequirements** model:
       - `task_name: Optional[str]` with confidence score
       - `description: Optional[str]` with confidence score
       - `target_table: Optional[str]` with confidence score
       - `objective: Optional[str]` with confidence score
       - `success_criteria: Optional[list[str]]` with confidence score
       - `constraints: Optional[list[str]]` with confidence score
       - `overall_confidence: float` (0.0-1.0)
       - `is_complete: bool` (all required fields filled)
    
    3. **ChatSession** model:
       - `session_id: str` (UUID)
       - `messages: list[ChatMessage]`
       - `created_at: datetime`
       - `updated_at: datetime`
       - `current_requirements: Optional[ExtractedRequirements]`
    
    Use existing patterns from aadap/models/ directory. Each field with confidence allows partial extraction display.
  </action>
  <verify>
    `python -c "from aadap.models.chat import ChatSession, ChatMessage, ExtractedRequirements; print('Models imported successfully')"`
  </verify>
  <done>
    Chat models exist with ChatSession, ChatMessage, and ExtractedRequirements classes. All required fields present with proper typing.
  </done>
</task>

<task type="auto">
  <name>Task 2: Create chat service with requirement extraction</name>
  <files>aadap/services/chat_service.py</files>
  <action>
    Create ChatService class that handles conversation and requirement extraction:
    
    1. **Session management**:
       - `create_session() -> ChatSession` — Create new session with UUID
       - `get_session(session_id: str) -> ChatSession` — Retrieve or raise 404
       - `sessions: dict[str, ChatSession]` — In-memory session storage (session scope, not persistent)
    
    2. **Streaming chat** (async generator):
       - `stream_response(session_id: str, user_message: str) -> AsyncGenerator[str, None]`
       - Append user message to session
       - Call Azure OpenAI with system prompt for requirement extraction
       - Stream response chunks as SSE-formatted strings (`data: {...}\n\n`)
       - After stream completes, parse response for extracted requirements
       - Update session.current_requirements
    
    3. **Requirement extraction prompt**:
       - System prompt instructs LLM to:
         - Have natural conversation to clarify requirements
         - Extract structured requirements as JSON when confident
         - Mark fields as uncertain if ambiguous
       - LLM returns: `{"message": "...", "requirements": {...}}`
    
    4. **Integration with existing LLM config**:
       - Use existing Azure OpenAI configuration from aadap/config.py
       - Use AsyncOpenAI client for streaming
       - Model: gpt-4o (same as orchestrator)
    
    DO NOT use LangGraph for chat — direct LLM calls are simpler for this use case.
    DO NOT persist sessions to database — in-memory for session scope only.
  </action>
  <verify>
    `python -c "from aadap.services.chat_service import ChatService; s = ChatService(); print('ChatService instantiated')"`
  </verify>
  <done>
    ChatService handles session creation, message streaming via Azure OpenAI, and extracts structured requirements. Sessions stored in-memory with UUID keys.
  </done>
</task>

<task type="auto">
  <name>Task 3: Create chat API endpoints with SSE</name>
  <files>aadap/api/chat.py</files>
  <action>
    Create FastAPI router for chat endpoints:
    
    1. **POST /api/chat/sessions** — Create new chat session
       - Returns: `{"session_id": "uuid", "created_at": "..."}`
       - Status: 201 Created
    
    2. **GET /api/chat/sessions/{session_id}** — Get session with history
       - Returns: ChatSession with all messages and current requirements
       - Status: 200 or 404 if not found
    
    3. **POST /api/chat/sessions/{session_id}/messages** — Send message, stream response
       - Request body: `{"message": "user input"}`
       - Response: SSE stream with content-type `text/event-stream`
       - Each SSE event: `data: {"type": "content"|"requirements", "content": "..."}\n\n`
       - Final event includes updated requirements
    
    4. **PATCH /api/chat/sessions/{session_id}/requirements** — Update requirements directly
       - Request body: `ExtractedRequirements` (partial update)
       - Allows user to edit requirements panel directly
       - Returns: updated ChatSession
    
    5. **POST /api/chat/sessions/{session_id}/create-task** — Create task from requirements
       - Validates requirements.is_complete is True (returns 400 if not)
       - Creates Task record using existing Task model
       - Returns: `{"task_id": "...", "redirect_url": "/tasks/{task_id}"}`
    
    Register router in main app (aadap/main.py) at prefix `/api/chat`.
    
    Use StreamingResponse for SSE endpoint with proper headers.
  </action>
  <verify>
    `curl -X POST http://localhost:8000/api/chat/sessions -H "Content-Type: application/json" 2>/dev/null | head -c 200`
  </verify>
  <done>
    Chat API endpoints exist at /api/chat/* with SSE streaming for message responses. Sessions can be created, retrieved, and used to create tasks.
  </done>
</task>

</tasks>

<verification>
1. Chat models import without errors
2. ChatService can create session and stream response
3. API endpoints respond correctly (sessions, messages, requirements, create-task)
4. SSE streaming works with proper event format
</verification>

<success_criteria>
- [ ] ChatSession, ChatMessage, ExtractedRequirements models defined
- [ ] ChatService handles multi-turn conversation with streaming
- [ ] Requirements extracted and updated during conversation
- [ ] API endpoints for session CRUD and SSE messaging
- [ ] Task creation from confirmed requirements works
- [ ] All endpoints registered and accessible
</success_criteria>

<output>
After completion, create `.planning/phases/02-chatbot/02-01-SUMMARY.md`
</output>
