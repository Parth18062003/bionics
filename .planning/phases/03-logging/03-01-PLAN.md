---
phase: 03-logging
plan: 01
type: execute
wave: 1
depends_on: []
files_modified: []
autonomous: true
requirements: [LOGS-03, LOGS-07]

must_haves:
  truths:
    - "Logs are stored in TaskLog table with timestamp, level, message, correlation_id"
    - "Orchestrator and agents emit structured logs"
    - "API endpoints exist to query logs"
  artifacts:
    - path: "aadap/services/log_service.py"
      provides: "Log service for querying and filtering task logs"
      exports: ["LogService"]
    - path: "aadap/api/routes/logs.py"
      provides: "Log API endpoints"
      exports: ["router"]
    - path: "aadap/db/models.py"
      provides: "TaskLog model (from Phase 1)"
      contains: "class TaskLog"
  key_links:
    - from: "aadap/api/routes/logs.py"
      to: "aadap/services/log_service.py"
      via: "dependency injection"
      pattern: "LogService"
    - from: "aadap/services/log_service.py"
      to: "aadap/db/models.py"
      via: "SQLAlchemy query"
      pattern: "TaskLog"
---

<objective>
Create the log service with structured emissions and API endpoints.

Purpose: Enable querying and filtering of task logs stored in the TaskLog table.
Output: Log service, API endpoints for log retrieval with filtering and search.
</objective>

<execution_context>
@C:/Users/parth/.config/opencode/get-shit-done/workflows/execute-plan.md
@C:/Users/parth/.config/opencode/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/03-logging/03-CONTEXT.md
@.planning/REQUIREMENTS.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Verify TaskLog model exists</name>
  <files>aadap/db/models.py</files>
  <action>
    Verify the TaskLog model from Phase 1 exists with the required fields:
    
    - `id`: UUID primary key
    - `task_id`: UUID foreign key to Task
    - `timestamp`: datetime
    - `level`: string (DEBUG, INFO, WARNING, ERROR)
    - `message`: text
    - `correlation_id`: UUID (optional, for tracing related logs)
    - `source`: string (agent name or service that emitted the log)
    
    If the model doesn't exist or is incomplete, add it. The model was created in Phase 1 as part of INFRA-01.
    
    Also verify the Task model has a relationship to TaskLog for easy access.
  </action>
  <verify>
    `python -c "from aadap.db.models import TaskLog; print(TaskLog.__table__.columns.keys())"`
  </verify>
  <done>
    TaskLog model exists with id, task_id, timestamp, level, message, correlation_id, source fields.
  </done>
</task>

<task type="auto">
  <name>Task 2: Create log service</name>
  <files>aadap/services/log_service.py</files>
  <action>
    Create LogService class for querying and filtering logs:
    
    1. **LogQueryParams** model:
       - `task_id`: Optional[UUID] — filter by task
       - `levels`: Optional[list[str]] — filter by multiple levels
       - `search`: Optional[str] — text search in message
       - `limit`: int = 100 — max results
       - `offset`: int = 0 — pagination offset
       - `start_time`: Optional[datetime] — filter from time
       - `end_time`: Optional[datetime] — filter to time
    
    2. **LogService** class:
       - `async get_logs(params: LogQueryParams, db: AsyncSession) -> list[TaskLog]`
         - Query TaskLog with filters
         - Support multi-level filter (WHERE level IN (...))
         - Support text search (WHERE message ILIKE '%search%')
         - Order by timestamp DESC
         - Apply limit/offset
       
       - `async get_log_count(params: LogQueryParams, db: AsyncSession) -> int`
         - Count matching logs for pagination
       
       - `async get_logs_by_correlation(correlation_id: UUID, db: AsyncSession) -> list[TaskLog]`
         - Get all logs with same correlation_id for tracing
       
       - `async export_logs(params: LogQueryParams, format: str, db: AsyncSession) -> str`
         - Export logs as JSON or CSV string
         - JSON: list of dicts with all fields
         - CSV: header row + data rows
       
       - `async get_recent_logs(task_id: UUID, limit: int, db: AsyncSession) -> list[TaskLog]`
         - Quick method for embedded view (last N logs for a task)
    
    Use existing SQLAlchemy patterns from aadap/db/models.py.
  </action>
  <verify>
    `python -c "from aadap.services.log_service import LogService, LogQueryParams; print('LogService loaded')"`
  </verify>
  <done>
    LogService exists with get_logs, get_log_count, get_logs_by_correlation, export_logs, get_recent_logs methods.
  </done>
</task>

<task type="auto">
  <name>Task 3: Create log API endpoints</name>
  <files>aadap/api/routes/logs.py</files>
  <action>
    Create FastAPI router for log endpoints:
    
    1. **GET /api/logs** — Query logs with filters
       - Query params: task_id, levels (comma-separated), search, limit, offset, start_time, end_time
       - Returns: `{ "logs": [...], "total": N, "limit": N, "offset": N }`
       - Status: 200
    
    2. **GET /api/logs/correlation/{correlation_id}** — Get logs by correlation ID
       - Returns all logs with same correlation_id for debugging
       - Returns: `{ "logs": [...], "correlation_id": "..." }`
       - Status: 200 or 404
    
    3. **GET /api/logs/export** — Export logs
       - Query params: same as GET /api/logs, plus format (json|csv)
       - Returns: file download with appropriate content-type
       - JSON: `application/json`
       - CSV: `text/csv` with Content-Disposition header
    
    4. **GET /api/tasks/{task_id}/logs** — Get recent logs for a task (embedded view)
       - Query params: limit (default 50)
       - Returns: `{ "logs": [...], "task_id": "..." }`
       - This is for the embedded minimal view in task detail page
    
    Register router in main app (aadap/main.py) at prefix `/api`.
    
    Use streaming response for large exports to avoid memory issues.
  </action>
  <verify>
    `curl -s "http://localhost:8000/api/logs?limit=5" | head -c 200`
  </verify>
  <done>
    Log API endpoints exist at /api/logs/* with filtering, search, and export. Embedded endpoint at /api/tasks/{id}/logs.
  </done>
</task>

<task type="auto">
  <name>Task 4: Add structured logging to orchestrator</name>
  <files>aadap/orchestrator/graph.py, aadap/orchestrator/graph_executor.py</files>
  <action>
    Add structured logging emissions to orchestrator that write to TaskLog table.
    
    1. **Create log emission helper**:
       - Add `emit_log(task_id, level, message, correlation_id, source)` function
       - Writes to TaskLog table via async session
       - Include correlation_id from task context for tracing
    
    2. **Instrument key orchestrator events**:
       - Task state transitions: "Task {task_id} transitioned from {old} to {new}"
       - Agent assignments: "Task {task_id} assigned to {agent_type}"
       - Graph execution start/end: "Graph execution started for task {task_id}"
       - Checkpoints reached: "Checkpoint {name} reached for task {task_id}"
       - Errors and retries: "Error in {step}: {error_message}"
    
    3. **Log levels**:
       - DEBUG: Detailed execution steps
       - INFO: State transitions, assignments, completions
       - WARNING: Retries, fallbacks, degraded performance
       - ERROR: Failures, exceptions
    
    Use existing logger patterns but also emit to TaskLog for UI visibility.
    
    DO NOT change existing logging behavior — add TaskLog emission alongside.
  </action>
  <verify>
    `grep -l "emit_log\|TaskLog" aadap/orchestrator/graph.py aadap/orchestrator/graph_executor.py`
  </verify>
  <done>
    Orchestrator emits structured logs to TaskLog table for all key events.
  </done>
</task>

</tasks>

<verification>
1. TaskLog model exists with required fields
2. LogService queries and filters logs correctly
3. API endpoints respond correctly with pagination
4. Export generates valid JSON and CSV
5. Orchestrator emits logs to TaskLog table
</verification>

<success_criteria>
- [ ] TaskLog model verified with correlation_id and source fields
- [ ] LogService exists with query, count, export methods
- [ ] API endpoints at /api/logs/* for querying and export
- [ ] Embedded endpoint at /api/tasks/{id}/logs for minimal view
- [ ] Orchestrator emits structured logs to TaskLog
</success_criteria>

<output>
After completion, create `.planning/phases/03-logging/03-01-SUMMARY.md`
</output>
